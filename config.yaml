# General Settings
general:
  train_data_size: 10                #-1 means unlimited games, where games are generated on the fly. In paper we use [1, 2, 10, 100, 500, -1], feel free to use any number.
  testset_path: "test_set"
  random_map: True                    # If True, map size is sampled from [2, 12], if False, then map size is fixed to be 6.
  question_type: "location"           # location, existence, attribute
  random_seed: 42                     # the secret of deep learning.
  naozi_capacity: 1                   # agent keeps this many history observations. In paper we do not use a size larger than 1, i.e., we do not keep history. Feel free to play with it, tho.
  use_cuda: False                     # disable this when running on machine without cuda

# Training Settings
training:
  batch_size: 1                       # Number of games played in parallel
  max_episode: 100000                 # Number of episodes to train
  learn_start_from_this_episode: 0    # Episodes of games played randomly to generate experience - only important for Value based methods
  target_net_update_frequency: 1000   # sync target net with online net per this many epochs - only important for Value based methods
  max_nb_steps_per_episode: 50        # after this many steps, a game is terminated
  qa_loss_lambda: 1.0                 # weights for QA loss value
  interaction_loss_lambda: 1.0        # weights for interaction loss value
  optimizer:
    step_rule: 'adam'  # adam
    learning_rate: 0.00025
    clip_grad_norm: 5

# Evaluation Settings
evaluate:
  run_eval: False                     # Run eval during training and save model by eval accuracy
  batch_size: 1
  max_nb_steps_per_episode: 50        # after this many steps, a game is terminated

checkpoint:
  save_checkpoint: True               # Save model checkpoints
  save_frequency: 1000                # episode
  experiment_tag: 'a2c_location_10_icm_inverse_only_no_feature_net'
  load_pretrained: True               # during test, enable this so that the agent load your pretrained model
  load_from_tag: ''  # if you want to load from prev experiment

replay:
  discount_gamma: 0.9
  replay_memory_capacity: 500000       # adjust this depending on your RAM size
  replay_memory_priority_fraction: 0.5
  update_per_k_game_steps: 20
  replay_batch_size: 64
  multi_step: 1

distributional:
  enable: False
  atoms: 51
  v_min: -10
  v_max: 10

# Use previous command word q values as input into network generating q values of next command word
dqn_conditioning: False

#use dueling networks
dueling_networks: False

# use DDQN
double_dqn: False


# Actor Critic Settings
a2c:
  enable: True
  entropy_coefficient: 0.05

# Intrinsic Curiosity Module Settings
icm:
  enable: True
  scaling_factor: 0.05
  beta: 0
  lambda: 1
  hidden_size: 128
  state_feature_size: 128
  inverse_reward: True
  use_intrinsic_reward: False
  use_feature_net : False


epsilon_greedy:
  noisy_net: False  # if this is true, then epsilon greedy is disabled
  epsilon_anneal_episodes: 100000  # -1 if not annealing
  epsilon_anneal_from: 1.0
  epsilon_anneal_to: 0.1

valid_command_bonus_lambda: 0.  # give reward when a generated command is in TextWorld's provided admissible command list. Note we don't use this in the paper.

episodic_counting_bonus:
  revisit_counting: True
  revisit_counting_lambda_anneal_from: 0.1
  revisit_counting_lambda_anneal_episodes: -1  # -1 if not annealing
  revisit_counting_lambda_anneal_to: 0.0

model:
  use_pretrained_embedding: True
  word_embedding_size: 300
  word_embedding_trainable: False
  char_embedding_size: 32
  char_embedding_trainable: True
  embedding_dropout: 0.
  encoder_layers: 1
  aggregation_layers: 3
  encoder_conv_num: 2
  aggregation_conv_num: 2
  block_hidden_dim: 64
  n_heads: 1
  attention_dropout: 0.
  block_dropout: 0.
  action_scorer_hidden_dim: 64
  question_answerer_hidden_dim: 64
