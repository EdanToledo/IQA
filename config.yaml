# General Settings
general:
  train_data_size: 1                  #-1 means unlimited games, where games are generated on the fly. In paper we use [1, 2, 10, 100, 500, -1], feel free to use any number.
  testset_path: "test_set"
  random_map: False                    # If True, map size is sampled from [2, 12], if False, then map size is fixed to be 6.
  question_type: "location"           # location, existence, attribute
  random_seed: 42                     # the secret of deep learning.
  naozi_capacity: 1                   # agent keeps this many history observations. In paper we do not use a size larger than 1, i.e., we do not keep history. Feel free to play with it, tho.
  use_cuda: True                     # disable this when running on machine without cuda
  
# Training Settings
training:
  batch_size: 1                       # Number of games played in parallel
  max_episode: 200000                 # Number of episodes to train
  learn_start_from_this_episode: 0    # Episodes of games played randomly to generate experience - only important for Value based methods
  target_net_update_frequency: 1000   # sync target net with online net per this many epochs - only important for Value based methods
  max_nb_steps_per_episode: 80        # after this many steps, a game is terminated
  qa_loss_lambda: 1.0                 # weights for QA loss value
  interaction_loss_lambda: 1.0        # weights for interaction loss value
  optimizer:
    step_rule: 'adam'  # adam
    learning_rate: 0.00025
    clip_grad_norm: 5

# Evaluation Settings
evaluate:
  run_eval: False                     # Run eval during training and save model by eval accuracy
  batch_size: 1
  max_nb_steps_per_episode: 80        # after this many steps, a game is terminated

checkpoint:
  save_checkpoint: True               # Save model checkpoints
  save_frequency: 1000                # episode
  experiment_tag: ''                 # name of experiment
  load_pretrained: False               # during test, enable this so that the agent load your pretrained model
  load_from_tag: ''  # if you want to load from prev experiment

replay:
  discount_gamma: 0.9
  replay_memory_capacity: 500000       # adjust this depending on your RAM size
  replay_memory_priority_fraction: 0.5
  update_per_k_game_steps: 20
  replay_batch_size: 64
  multi_step: 1

distributional:
  enable: False
  atoms: 51
  v_min: -10
  v_max: 10

# Use previous command word q values as input into network generating q values of next command word
dqn_conditioning: False

#use dueling networks
dueling_networks: False

# use DDQN
double_dqn: False

# Actor Critic Settings
a2c:
  enable: True
  entropy_coefficient: 0.05  # entropy coefficient used to weight entropy in loss
  condition_commands : True  # use previous word in command triplet to calculate next word

# Intrinsic Curiosity Module Settings
icm:
  enable: False
  beta: 0                     # proportion of inverse vs forward model loss: (1-beta)*inverse + beta*forward
  lambda: 1                   # weight of non-icm loss
  icm_loss_weight : 1         # weight of icm loss
  hidden_size: 128            # hidden size
  state_feature_size: 128     # if using feature net - size
  use_feature_net : False     # choice to use feature net or not
  inverse_action_weight: 1    # weight of inverse model action loss
  inverse_modifier_weight: 1  # weight of inverse model modifier loss
  inverse_object_weight: 1    # weight of inverse model object loss
  use_intrinsic_reward: False # use error as intrinsic motivation
  reward_scaling_factor: 0    # scale of error for reward
  inverse_reward: True        # use inverse models error as reward instead of forward model
  condition_decoding: False   # use previous decoded word in decoding of future words in command triplet
 
  
epsilon_greedy:
  noisy_net: False  # if this is true, then epsilon greedy is disabled
  epsilon_anneal_episodes: 100000  # -1 if not annealing
  epsilon_anneal_from: 1.0
  epsilon_anneal_to: 0.1

valid_command_bonus_lambda: 0.  # give reward when a generated command is in TextWorld's provided admissible command list. Note we don't use this in the paper.

episodic_counting_bonus:
  revisit_counting: True
  revisit_counting_lambda_anneal_from: 0.1
  revisit_counting_lambda_anneal_episodes: -1  # -1 if not annealing
  revisit_counting_lambda_anneal_to: 0.0

model:
  use_pretrained_embedding: True
  word_embedding_size: 300
  word_embedding_trainable: False
  char_embedding_size: 32
  char_embedding_trainable: True
  embedding_dropout: 0.
  encoder_layers: 1
  aggregation_layers: 3
  encoder_conv_num: 2
  aggregation_conv_num: 2
  block_hidden_dim: 64
  n_heads: 1
  attention_dropout: 0.
  block_dropout: 0.
  action_scorer_hidden_dim: 64
  question_answerer_hidden_dim: 64
  freeze_encoding: False # freeze weights for encoder